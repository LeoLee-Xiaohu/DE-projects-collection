{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1  ->  <Response [200]>\n",
      "2  ->  <Response [200]>\n",
      "3  ->  <Response [200]>\n",
      "4  ->  <Response [200]>\n",
      "5  ->  <Response [200]>\n",
      "6  ->  <Response [200]>\n",
      "7  ->  <Response [200]>\n",
      "8  ->  <Response [200]>\n",
      "9  ->  <Response [200]>\n",
      "10  ->  <Response [200]>\n",
      "11  ->  <Response [200]>\n",
      "12  ->  <Response [200]>\n",
      "13  ->  <Response [200]>\n",
      "14  ->  <Response [200]>\n",
      "15  ->  <Response [200]>\n",
      "16  ->  <Response [200]>\n",
      "17  ->  <Response [200]>\n",
      "18  ->  <Response [200]>\n",
      "19  ->  <Response [200]>\n",
      "20  ->  <Response [200]>\n",
      "21  ->  <Response [200]>\n",
      "22  ->  <Response [200]>\n",
      "23  ->  <Response [200]>\n",
      "24  ->  <Response [200]>\n",
      "25  ->  <Response [200]>\n",
      "26  ->  <Response [200]>\n",
      "27  ->  <Response [200]>\n",
      "28  ->  <Response [200]>\n",
      "29  ->  <Response [200]>\n",
      "30  ->  <Response [200]>\n",
      "31  ->  <Response [200]>\n",
      "32  ->  <Response [200]>\n",
      "33  ->  <Response [200]>\n",
      "34  ->  <Response [200]>\n",
      "35  ->  <Response [200]>\n",
      "36  ->  <Response [200]>\n",
      "37  ->  <Response [200]>\n",
      "38  ->  <Response [200]>\n",
      "39  ->  <Response [200]>\n",
      "40  ->  <Response [200]>\n",
      "41  ->  <Response [200]>\n",
      "42  ->  <Response [200]>\n",
      "43  ->  <Response [200]>\n",
      "44  ->  <Response [200]>\n",
      "45  ->  <Response [200]>\n",
      "46  ->  <Response [200]>\n",
      "47  ->  <Response [200]>\n",
      "48  ->  <Response [200]>\n",
      "49  ->  <Response [200]>\n",
      "50  ->  <Response [200]>\n",
      "51  ->  <Response [200]>\n",
      "52  ->  <Response [200]>\n",
      "53  ->  <Response [200]>\n",
      "54  ->  <Response [200]>\n",
      "55  ->  <Response [200]>\n",
      "56  ->  <Response [200]>\n",
      "57  ->  <Response [200]>\n",
      "58  ->  <Response [200]>\n",
      "59  ->  <Response [200]>\n",
      "60  ->  <Response [200]>\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time \n",
    "\n",
    "\n",
    "# Function to extract Product Title\n",
    "def get_title(soup):\n",
    "\n",
    "    try:\n",
    "        # Outer Tag Object\n",
    "        title = soup.find(\"span\", attrs={\"id\":'productTitle'})\n",
    "        \n",
    "        # Inner NavigatableString Object\n",
    "        title_value = title.text\n",
    "\n",
    "        # Title as a string value\n",
    "        title_string = title_value.strip()\n",
    "\n",
    "    except AttributeError:\n",
    "        title_string = \"\"\n",
    "\n",
    "    return title_string\n",
    "\n",
    "# Function to extract Product Price\n",
    "def get_price(soup):\n",
    "\n",
    "    try:\n",
    "        price = soup.find(\"span\", attrs={'id':'priceblock_ourprice'}).string.strip()\n",
    "\n",
    "    except AttributeError:\n",
    "\n",
    "        try:\n",
    "            # If there is some deal price\n",
    "            price = soup.find(\"span\", attrs={'id':'priceblock_dealprice'}).string.strip()\n",
    "\n",
    "        except:\n",
    "            price = \"\"\n",
    "\n",
    "    return price\n",
    "\n",
    "# Function to extract Product Rating\n",
    "def get_rating(soup):\n",
    "\n",
    "    try:\n",
    "        rating = soup.find(\"i\", attrs={'class':'a-icon a-icon-star a-star-4-5'}).string.strip()\n",
    "    \n",
    "    except AttributeError:\n",
    "        try:\n",
    "            rating = soup.find(\"span\", attrs={'class':'a-icon-alt'}).string.strip()\n",
    "        except:\n",
    "            rating = \"\"\t\n",
    "\n",
    "    return rating\n",
    "\n",
    "# Function to extract Number of User Reviews\n",
    "def get_review_count(soup):\n",
    "    try:\n",
    "        review_count = soup.find(\"span\", attrs={'id':'acrCustomerReviewText'}).string.strip()\n",
    "\n",
    "    except AttributeError:\n",
    "        review_count = \"\"\t\n",
    "\n",
    "    return review_count\n",
    "\n",
    "# Function to extract Availability Status\n",
    "def get_availability(soup):\n",
    "    try:\n",
    "        available = soup.find(\"div\", attrs={'id':'availability'})\n",
    "        available = available.find(\"span\").string.strip()\n",
    "\n",
    "    except AttributeError:\n",
    "        available = \"Not Available\"\t\n",
    "\n",
    "    return available\n",
    "\n",
    "def scraping(URL, HEADERS):\n",
    "\n",
    "    webpage = requests.get(URL, headers = HEADERS)\n",
    "    soup = BeautifulSoup(webpage.content, \"html.parser\")\n",
    "    s = soup.find_all(\"a\", attrs={'class':\"a-link-normal s-underline-text s-underline-link-text s-link-style a-text-normal\" })\n",
    "\n",
    "    n = 1\n",
    "    d = {\"title\":[], \"price\":[], \"rating\":[], \"reviews\":[],\"availability\":[]}\n",
    "\n",
    "    for i in s:\n",
    "\n",
    "        link_n = \"https://www.amazon.com.au\" + i.get('href')\n",
    "        new_webpage = requests.get(link_n, headers = HEADERS )\n",
    "        print(n,\" -> \", new_webpage)\n",
    "        n += 1\n",
    "        new_soup = BeautifulSoup(new_webpage.content,\"html.parser\")\n",
    "        time.sleep(0.35)\n",
    "\n",
    "\n",
    "            # Function calls to display all necessary product information\n",
    "        d['title'].append(get_title(new_soup))\n",
    "        d['price'].append(get_price(new_soup))\n",
    "        d['rating'].append(get_rating(new_soup))\n",
    "        d['reviews'].append(get_review_count(new_soup))\n",
    "        d['availability'].append(get_availability(new_soup))\n",
    "        \n",
    "\n",
    "    \n",
    "    return d\n",
    "\n",
    "def transform(diction):\n",
    "    dataframe = pd.DataFrame.from_dict(diction)\n",
    "    dataframe['title'].replace(\"\",np.nan, inplace= True)\n",
    "    dataframe = dataframe.dropna(subset='title')\n",
    "    dataframe.to_csv('iphone14_pro_cases2.csv', header= True, index= False)\n",
    "\n",
    "# def main():\n",
    "\n",
    "#     URL = \"https://www.amazon.com.au/s?k=iphone+14+pro+case&crid=393IQZJ88FNQS&sprefix=%2Caps%2C238&ref=nb_sb_ss_recent_1_0_recent\"\n",
    "\n",
    "#     HEADERS = ({'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Safari/537.36', 'Accept-Language':'en-US, en;q=0.5'})\n",
    "\n",
    "#     dataframe = scraping(URL, HEADERS)\n",
    "#     transform(dataframe)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    URL = \"https://www.amazon.com.au/s?k=iphone+14+pro+case&crid=393IQZJ88FNQS&sprefix=%2Caps%2C238&ref=nb_sb_ss_recent_1_0_recent\"\n",
    "    HEADERS = ({'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Safari/537.36', 'Accept-Language':'en-US, en;q=0.5'})\n",
    "\n",
    "    data = scraping(URL, HEADERS)\n",
    "    transform(data)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
