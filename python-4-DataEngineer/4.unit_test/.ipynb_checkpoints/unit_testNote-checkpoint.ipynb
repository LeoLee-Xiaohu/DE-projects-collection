{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9ce0633",
   "metadata": {},
   "source": [
    "# 1.1 unit test 可以大大节约手动检查程序的时间\n",
    "\n",
    "### unit test就是一种手动或自动检查函数或模块单元，能否在各种不用的输入后，能够输出正确的输出结果的程序测试方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1023b92",
   "metadata": {},
   "source": [
    "# 1.2 unit test building process\n",
    "\n",
    "### step1. create file （test_filename 包括函数、模块）\n",
    "### step2. import pytest 和 test module filename.\n",
    "### step3. unit test python function:    def test_XX():\n",
    "### step4. assert\n",
    "### step5. run unit test in command line : !pytest test_filename.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7687ceeb",
   "metadata": {},
   "source": [
    "# 例如测试模块以下输入是否得到设定输出\n",
    "<img src='./chapter1.png' >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b975f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the pytest package\n",
    "import pytest\n",
    "\n",
    "import row_to_list\n",
    "\n",
    "def test_for_clean_row():\n",
    "    assert row_to_list(\"2,081\\t314,942\\n\") == \\\n",
    "         [\"2,081\", \"314,942\"]\n",
    "    \n",
    "def test_for_missing_area():\n",
    "    assert row_to_list(\"\\t293,410\\n\") is None\n",
    "    \n",
    "def test_for_missing_tab():\n",
    "    assert row_to_list(\"1,463238,765\\n\") is None\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe7c1b1",
   "metadata": {},
   "source": [
    "# 1.3 查看test result\n",
    "\n",
    "## F代表failed，有bug， .代表成功\n",
    "\n",
    "### If you get an AssertionError, this means the function has a bug and you should fix it. If you get another exception, e.g. NameError, this means that something else is wrong with the unit test code and you should fix it so that the assert statement can actually run. 也会在TDD中出现，被测试code还没写完。\n",
    "\n",
    "To find bugs in functions, you need to follow a four step procedure.\n",
    "\n",
    "1.Write unit tests.\n",
    "\n",
    "2. Run them.\n",
    "\n",
    "3.Read the test result report and spot the bugs.\n",
    "\n",
    "4.Fix the bugs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9cfcc1",
   "metadata": {},
   "source": [
    "# 2.1 test message, 将test报告修饰可读\n",
    "\n",
    "### pytest 不可检测小数，float要用pytest.approx()方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fe5002",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "from preprocessing_helpers import convert_to_int\n",
    "\n",
    "def test_on_string_with_one_comma():\n",
    "    test_argument = \"2,081\"\n",
    "    expected = 2081\n",
    "    actual = convert_to_int(test_argument)\n",
    "    # Format the string with the actual return value\n",
    "    message = \"convert_to_int('2,081') should return the int 2081, but it actually returned {0}\".format(actual)\n",
    "    # Write the assert statement which prints message on failure\n",
    "    assert actual is expected, message "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8854a045",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pytest\n",
    "from as_numpy import get_data_as_numpy_array\n",
    "\n",
    "def test_on_clean_file():\n",
    "    expected = np.array([[2081.0, 314942.0],\n",
    "                       [1059.0, 186606.0],\n",
    "  \t\t\t\t\t   [1148.0, 206186.0]\n",
    "                       ]\n",
    "                      )\n",
    "    actual = get_data_as_numpy_array(\"example_clean_data.txt\", num_columns=2)\n",
    "    message = \"Expected return value: {0}, Actual return value: {1}\".format(expected, actual)\n",
    "  # Complete the assert statement\n",
    "    assert actual == pytest.approx(actual), message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75725211",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_on_six_rows():\n",
    "    example_argument = np.array([[2081.0, 314942.0], [1059.0, 186606.0],\n",
    "                                 [1148.0, 206186.0], [1506.0, 248419.0],\n",
    "                                 [1210.0, 214114.0], [1697.0, 277794.0]]\n",
    "                                )\n",
    "    # Fill in with training array's expected number of rows\n",
    "    expected_training_array_num_rows = 4\n",
    "    # Fill in with testing array's expected number of rows\n",
    "    expected_testing_array_num_rows = 2\n",
    "    actual = split_into_training_and_testing_sets(example_argument)\n",
    "    # Write the assert statement checking training array's number of rows\n",
    "    assert actual[0].shape[0] == expected_training_array_num_rows, \"The actual number of rows in the training array is not {}\".format(expected_training_array_num_rows)\n",
    "    # Write the assert statement checking testing array's number of rows\n",
    "    assert actual[1].shape[0] == 2, \"The actual number of rows in the testing array is not {}\".format(expected_testing_array_num_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7820de72",
   "metadata": {},
   "source": [
    "# 2.2 pytest.raises() \n",
    "\n",
    "### pytest.raises() 一种context manager 测试model是否能检测出exception 的bug。\n",
    "### 如果能检测出exception 的bug，则测试通过。如果不能，则测试返回failed，测试失败，需要修复bug。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec05a0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    " def test_valueerror_on_one_dimensional_argument():\n",
    "    example_argument = np.array([2081, 314942, 1059, 186606, 1148, 206186])\n",
    "    #用pytest.raises(ValueError) 上下文管理器\n",
    "    with pytest.raises(ValueError) as exception_info:    # store the exception\n",
    "        split_into_training_and_testing_sets(example_argument)\n",
    "    # Check if ValueError contains correct message，能报ValueError的错并与错误信息匹配则通过，不能则不通过测试\n",
    "    assert exception_info.match(\"Argument data array must be two dimensional. \"\n",
    "                                \"Got 1 dimensional array instead!\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19f3f80",
   "metadata": {},
   "source": [
    "# 2.3 test arguments\n",
    "\n",
    "## 多少argument需要被test？\n",
    "### 1.bad arguments 会返回exception的那种\n",
    "### 2. special arguments (boundary values, 控制函数行为的错误值，例如training set which is not 0.75 of total dataset as we set. Normal 值紧挨着的错误值边界值。)\n",
    "### 3. normal arguments.\n",
    "\n",
    "例在case row_to_list 的检测中： The boundary values of row_to_list() are now marked in orange. The normal argument is marked in green and the values triggering special behavior are marked in blue.\n",
    "<img src='./2.3test_arguments.png'  >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922cb97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''测试special arguments. (boundary values )'''\n",
    "\n",
    "import pytest\n",
    "from preprocessing_helpers import row_to_list\n",
    "\n",
    "def test_on_no_tab_no_missing_value():    # (0, 0) boundary value， （1，0）是正常值\n",
    "    # Assign actual to the return value for the argument \"123\\n\"\n",
    "    actual = row_to_list(\"123\\n\")\n",
    "    assert actual is None, \"Expected: None, Actual: {0}\".format(actual)\n",
    "    \n",
    "def test_on_two_tabs_no_missing_value():    # (2, 0) boundary value\n",
    "    actual = row_to_list(\"123\\t4,567\\t89\\n\")\n",
    "    # Complete the assert statement\n",
    "    assert actual is None, \"Expected: None, Actual: {0}\".format(actual)\n",
    "    \n",
    "def test_on_one_tab_with_missing_value():    # (1, 1) boundary value\n",
    "    actual = row_to_list(\"\\t4,567\\n\")\n",
    "    # Format the failure message\n",
    "    assert actual is None, \"Expected: None, Actual: {0}\".format(actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e0fab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''测试 bad arguments '''\n",
    "\n",
    "import pytest\n",
    "from preprocessing_helpers import row_to_list\n",
    "\n",
    "def test_on_no_tab_with_missing_value():    # (0, 1) case\n",
    "    # Assign to the actual return value for the argument \"\\n\"\n",
    "    actual = row_to_list('\\n')\n",
    "    # Write the assert statement with a failure message\n",
    "    assert actual is None , \"Expected: None, Actual: {0}\".format(actual)\n",
    "    \n",
    "def test_on_two_tabs_with_missing_value():    # (2, 1) case\n",
    "    # Assign to the actual return value for the argument \"123\\t\\t89\\n\"\n",
    "    actual = row_to_list('123\\t\\t89\\n')\n",
    "    # Write the assert statement with a failure message\n",
    "    assert actual is None, \"Expected: None, Actual: {0}\".format(actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a019df11",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''测试normal arguments'''\n",
    "\n",
    "import pytest\n",
    "from preprocessing_helpers import row_to_list\n",
    "\n",
    "def test_on_normal_argument_1():\n",
    "    actual = row_to_list(\"123\\t4,567\\n\")\n",
    "    # Fill in with the expected return value for the argument \"123\\t4,567\\n\"\n",
    "    expected = [\"123\", \"4,567\"]\n",
    "    assert actual == expected, \"Expected: {0}, Actual: {1}\".format(expected, actual)\n",
    "    \n",
    "def test_on_normal_argument_2():\n",
    "    actual = row_to_list(\"1,059\\t186,606\\n\")\n",
    "    expected = [\"1,059\", \"186,606\"]\n",
    "    # Write the assert statement along with a failure message\n",
    "    assert expected == actual, \"Expected: {0}, Actual: {1}\".format(expected, actual)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219e6f4b",
   "metadata": {},
   "source": [
    "# 2.4 TDD (test driven development)\n",
    "\n",
    "## 在软件开发的时候，先编写test 代码，再写软件模块程序（根据bug写程序），再对模块程序进行测试"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab87461",
   "metadata": {},
   "source": [
    "In TDD, the first run of the tests always fails with a NameError or ImportError because the function does not exist yet. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9126bc",
   "metadata": {},
   "source": [
    "# 3.1 test directory, test class\n",
    "\n",
    "## test class 就是一个test function的container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b83791",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "import numpy as np\n",
    "\n",
    "from models.train import split_into_training_and_testing_sets\n",
    "\n",
    "# Declare the test class\n",
    "class TestSplitIntoTrainingAndTestingSets(object ):\n",
    "    # Fill in with the correct mandatory argument\n",
    "    def test_on_one_row(self):\n",
    "        test_argument = np.array([[1382.0, 390167.0]])\n",
    "        with pytest.raises(ValueError) as exc_info:\n",
    "            split_into_training_and_testing_sets(test_argument)\n",
    "        expected_error_msg = \"Argument data_array must have at least 2 rows, it actually has just 1\"\n",
    "        assert exc_info.match(expected_error_msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390be852",
   "metadata": {},
   "source": [
    "# 3.2 shell command for all test file\n",
    "\n",
    "#### 1. 测试当前文件夹下所有的test文件，!pytest; or !pytest -x 整个test包(碰到报错代码就停止测试）。 In real life, the !pytest or !pytest -x command is often used in CI servers. It can also be useful if there is a major update to the code base, which changes many application modules at the same time. Running all tests is the only way to check if anything was broken due to the update.\n",
    "#### 2. 测试class， path+\" :: \" + classname。 !pytest models/test_train.py::TestSplitIntoTrainingAndTestingSets\n",
    "#### 3.测试method，path+\" :: \" + classname+\" :: \" + methodname  。 !pytest models/test_train.py::TestSplitIntoTrainingAndTestingSets::test_on_six_rows\n",
    "#### 4.关键字查找!pytest -k。!pytest -k \"SplitInto\"。 The -k flag is really useful, because it helps you select tests and test classes by typing only a unique part of its name. This saves a lot of typing, and you must admit that\n",
    "#### 5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c0e6e7",
   "metadata": {},
   "source": [
    "# 3.3 Mark a test class as expected to fail\n",
    "\n",
    "#### 使用@pytest.mark.xfail(reason=\"Using TDD, model_test() has not yet been implemented\")，隐藏module还没建好报NameError的错误，使其余部分通过测试。==> 查看报告原因，shell command: !pytest -rx\n",
    "#### 使用@pytest.mark.skipif(sys.version_info > (2, 7), reason=\"Works only on Python 2.7 or lower\")，跳过版本问题的错误.==> shell command: !pytest -rs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff880a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a reason for the expected failure\n",
    "@pytest.mark.xfail(reason=\"Using TDD, model_test() has not yet been implemented\")\n",
    "class TestModelTest(object):\n",
    "    def test_on_linear_data(self):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f983fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the sys module\n",
    "import sys\n",
    "\n",
    "class TestGetDataAsNumpyArray(object):\n",
    "    # Add a reason for skipping the test\n",
    "    @pytest.mark.skipif(sys.version_info > (2, 7), reason=\"Works only on Python 2.7 or lower\")\n",
    "    def test_on_clean_file(self):\n",
    "        expected = np.array([[2081.0, 314942.0],\n",
    "                             [1059.0, 186606.0],\n",
    "                             [1148.0, 206186.0]\n",
    "                             ]\n",
    "                            )\n",
    "        actual = get_data_as_numpy_array(\"example_clean_data.txt\", num_columns=2)\n",
    "        message = \"Expected return value: {0}, Actual return value: {1}\".format(expected, actual)\n",
    "        assert actual == pytest.approx(expected), message"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911588d4",
   "metadata": {},
   "source": [
    "# 3.4 travel CI 的安装与使用，上传GitHub检测代码"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea00d047",
   "metadata": {},
   "source": [
    "# 4.1 test preprocessing functions --fixture\n",
    "### fixture. pytest keeps the fixtures separate from the tests as this encourages reusing fixtures for tests that need the same/similar setup and teardown code.\n",
    "\n",
    "#### Write a fixture for an empty data file\n",
    "When a function takes a data file as an argument, you need to write a fixture that takes care of creating and deleting that data file. \n",
    "\n",
    "1. Creates an empty data file empty.txt relative to the current working directory in setup.\n",
    "\n",
    "2. Yields the path to the empty data file.\n",
    "\n",
    "3. Deletes the empty data file in teardown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de978cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pytest.fixture\n",
    "def empty_file():\n",
    "    # Assign the file path \"empty.txt\" to the variable\n",
    "    file_path = 'empty.txt'\n",
    "    open(file_path, \"w\").close()\n",
    "    # Yield the variable file_path\n",
    "    yield file_path\n",
    "    # Remove the file in the teardown\n",
    "    os.remove(file_path)\n",
    "    \n",
    "def test_on_empty_file(self, empty_file):\n",
    "    expected = np.empty((0, 2))\n",
    "    actual = get_data_as_numpy_array(empty_file, 2)\n",
    "    assert actual == pytest.approx(expected), \"Expected: {0}, Actual: {1}\".format(expected, actual)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d302e728",
   "metadata": {},
   "source": [
    "### Fixture chaining using tmpdir\n",
    "The built-in tmpdir fixture is very useful when dealing with files in setup and teardown. tmpdir combines seamlessly with user defined fixture via fixture chaining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605a0615",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "\n",
    "@pytest.fixture\n",
    "# Add the correct argument so that this fixture can chain with the tmpdir fixture\n",
    "def empty_file(tmpdir):\n",
    "    # Use the appropriate method to create an empty file in the temporary directory\n",
    "    file_path = tmpdir.join(\"empty.txt\")\n",
    "    open(file_path, \"w\").close()\n",
    "    yield file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cee590",
   "metadata": {},
   "source": [
    "# 4.2 mocking \n",
    "### 屏蔽test通过不了的部分（函数或模块），使其成为bug-free。暂时通过test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728d8787",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Program a bug-free dependency'''\n",
    "\n",
    "# Define a function convert_to_int_bug_free\n",
    "def convert_to_int_bug_free(comma_separated_integer_string):\n",
    "    # Assign to the dictionary holding the correct return values \n",
    "    return_values = {\"1,801\": 1801, \"201,411\": 201411, \"2,002\": 2002, \"333,209\": 333209, \"1990\": None, \"782,911\": 782911, \"1,285\": 1285, \"389129\": None}\n",
    "    # Return the correct result using the dictionary return_values\n",
    "    return return_values[comma_separated_integer_string]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56651864",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Mock a dependency'''\n",
    "\n",
    "# Add the correct argument to use the mocking fixture in this test\n",
    "def test_on_raw_data(self, raw_and_clean_data_file, mocker):\n",
    "    raw_path, clean_path = raw_and_clean_data_file\n",
    "    # Replace the dependency with the bug-free mock\n",
    "    convert_to_int_mock = mocker.patch(\"data.preprocessing_helpers.convert_to_int\",\n",
    "                                       side_effect=convert_to_int_bug_free)\n",
    "    preprocess(raw_path, clean_path)\n",
    "    # Check if preprocess() called the dependency correctly\n",
    "    assert convert_to_int_mock.call_args_list == [call(\"1,801\"), call(\"201,411\"), call(\"2,002\"), call(\"333,209\"), call(\"1990\"), call(\"782,911\"), call(\"1,285\"), call(\"389129\")]\n",
    "    with open(clean_path, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "    first_line = lines[0]\n",
    "    assert first_line == \"1801\\\\t201411\\\\n\"\n",
    "    second_line = lines[1]\n",
    "    assert second_line == \"2002\\\\t333209\\\\n\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d74d0a",
   "metadata": {},
   "source": [
    "# 4.3 testing models\n",
    "\n",
    "### 检测模型是否能运行\n",
    "#### The model_test() function, which measures how well the model fits unseen data, returns a quantity called  which is very difficult to compute in the general case. Therefore, you need to find special testing sets where computing  is easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabfcfc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''case. 1. leaner model'''\n",
    "\n",
    "import numpy as np\n",
    "import pytest\n",
    "from models.train import model_test\n",
    "\n",
    "def test_on_perfect_fit():\n",
    "    # Assign to a NumPy array containing a linear testing set\n",
    "    test_argument = np.array([[1.0, 3.0], [2.0, 5.0], [3.0, 7.0]])\n",
    "    # Fill in with the expected value of r^2 in the case of perfect fit\n",
    "    expected = 1.0\n",
    "    # Fill in with the slope and intercept of the model\n",
    "    actual = model_test(test_argument, slope=2.0, intercept=1.0)\n",
    "    # Complete the assert statement\n",
    "    assert actual == pytest.approx(expected), \"Expected: {0}, Actual: {1}\".format(expected, actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5373a10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''case. 2 '''\n",
    "\n",
    "def test_on_circular_data(self):\n",
    "    theta = pi/4.0\n",
    "    # Assign to a NumPy array holding the circular testing data\n",
    "    test_argument = np.array([[1.0, 0.0], [cos(theta), sin(theta)],\n",
    "                              [0.0, 1.0],\n",
    "                              [cos(3 * theta), sin(3 * theta)],\n",
    "                              [-1.0, 0.0],\n",
    "                              [cos(5 * theta), sin(5 * theta)],\n",
    "                              [0.0, -1.0],\n",
    "                              [cos(7 * theta), sin(7 * theta)]]\n",
    "                             )\n",
    "    # Fill in with the slope and intercept of the straight line\n",
    "    actual = model_test(test_argument, slope=0.0, intercept=0.0)\n",
    "    # Complete the assert statement\n",
    "    assert actual == pytest.approx(0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d952e017",
   "metadata": {},
   "source": [
    "# 4.4 plot test image"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
