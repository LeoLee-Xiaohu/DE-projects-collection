{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db291a36",
   "metadata": {},
   "source": [
    "# 1.0 è¯»å–CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6115e3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data frame of next 500 rows with labeled columns\n",
    "vt_data_next500 = pd.read_csv(\"vt_tax_data_2016.csv\", \n",
    "                       \t\t  nrows = 500,\n",
    "                       \t\t  skiprows = 500,\n",
    "                       \t\t  header = None,\n",
    "\t\t\t\t\t\t\t\tnames = list(vt_data_first500.columns)\n",
    "                       \t\t  )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f814825",
   "metadata": {},
   "source": [
    "##### Set custom NA values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae72f8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dict specifying that 0s in zipcode are NA values\n",
    "null_values = {'zipcode': 0}\n",
    "\n",
    "# Load csv using na_values keyword argument\n",
    "data = pd.read_csv(\"vt_tax_data_2016.csv\", \n",
    "                   na_values = null_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5472ad4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tax_data = pd.read_csv(\"us_tax_data_2016.csv\",\n",
    "                       na_values={\"zipcode\" : 0})\n",
    "print(tax_data[tax_data.zipcode.isna()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955ca23e",
   "metadata": {},
   "source": [
    "##### error_bad_lines=False, è·³è¿‡ä¸èƒ½è¯»å– parseçš„è¡Œ\n",
    "##### warn_bad_lines=Trueï¼Œ  æŸ¥çœ‹è·³è¿‡äº†å¤šå°‘è¡Œï¼Œæ˜¾ç¤ºbadè¡Œçš„ä¿¡æ¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99595a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tax_data = pd.read_csv(\"us_tax_data_2016_corrupt.csv\",\n",
    "                       error_bad_lines=False,\n",
    "                       warn_bad_lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5ec10c",
   "metadata": {},
   "source": [
    "# 2.1 è¯»å–è¡¨æ ¼excel sheet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba8263f",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Get the second sheet by position index\n",
    "survey_data_sheet2 = pd.read_excel('fcc_survey.xlsx',\n",
    "                                   sheet_name=1)\n",
    "# Get the second sheet by name\n",
    "survey_data_2017 = pd.read_excel('fcc_survey.xlsx',\n",
    "                                 sheet_name='2017')\n",
    "print(survey_data_sheet2.equals(survey_data_2017))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7eb051",
   "metadata": {},
   "source": [
    "\n",
    "# 2.2 boolå€¼è¯»å–åŠè®¾ç½®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9fb2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load file with Yes as a True value and No as a False value\n",
    "survey_subset = pd.read_excel(\"fcc_survey_yn_data.xlsx\",\n",
    "                              dtype={\"HasDebt\": bool,\n",
    "                              \"AttendedBootCampYesNo\": bool},\n",
    "                              true_values = ['Yes'],\n",
    "                              false_values = ['No'])\n",
    "\n",
    "# View the data\n",
    "print(survey_subset.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc5be1d",
   "metadata": {},
   "source": [
    "# 2.3 è½¬åŒ–æˆæ—¶é—´æ ¼å¼ pars_dates = [] or {å­—å…¸}\n",
    "\n",
    "## pd.to_datetime() to convert strings(non-standard datetime) to dates  ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720e1c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dict of columns to combine into new datetime column\n",
    "datetime_cols = {\"Part2Start\": ['Part2StartDate', 'Part2StartTime']}\n",
    "\n",
    "\n",
    "# Load file, supplying the dict to parse_dates\n",
    "survey_data = pd.read_excel(\"fcc_survey_dts.xlsx\",\n",
    "                            parse_dates = datetime_cols)\n",
    "\n",
    "\n",
    "survey_data[\"Part2EndTime\"] = pd.to_datetime(survey_data[\"Part2EndTime\"], \n",
    "                                             format=\"%m%d%Y %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953d1c87",
   "metadata": {},
   "source": [
    "# 3.0 é“¾æ¥æ•°æ®åº“\n",
    "\n",
    "#### ç”¨sqlalchemyåŒ…, é“¾æ¥å¹¶åˆ›å»ºSQLå¼•æ“ create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0735704e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "\n",
    "\n",
    "# Create the database engine\n",
    "engine = create_engine(\"sqlite:///data.db\")\n",
    "\n",
    "# Create a SQL query to load the entire weather table\n",
    "query = \"\"\"\n",
    "SELECT * \n",
    "  FROM weather;\n",
    "\"\"\"\n",
    "\n",
    "# Load weather with the SQL query\n",
    "weather = pd.read_sql(query, engine)\n",
    "\n",
    "# View the first few rows of data\n",
    "print(weather.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0dea8a",
   "metadata": {},
   "source": [
    "# 4.0 JSONæ–‡ä»¶çš„æ¥å…¥"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1b7c26",
   "metadata": {},
   "source": [
    "## 4.1 pd.read_json( 'æ–‡ä»¶å.json' , orient = 'split')\n",
    "\n",
    "ç¬¬ä¸€å‚æ•°å°±æ˜¯jsonæ–‡ä»¶è·¯å¾„æˆ–è€…jsonæ ¼å¼çš„å­—ç¬¦ä¸²ã€‚\n",
    "\n",
    "ç¬¬äºŒå‚æ•°orientæ˜¯è¡¨æ˜é¢„æœŸçš„jsonå­—ç¬¦ä¸²æ ¼å¼, orient = åŒ…æ‹¬â€œsplit\", 'index', 'records', 'columns', 'values' è¯¦ç»†æŸ¥çœ‹ http://www.manongjc.com/article/50826.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2e00bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Load the JSON with orient specified\n",
    "    df = pd.read_json(\"dhs_report_reformatted.json\",\n",
    "                      orient = 'split')\n",
    "    \n",
    "    # Plot total population in shelters over time\n",
    "    df[\"date_of_census\"] = pd.to_datetime(df[\"date_of_census\"])\n",
    "    df.plot(x=\"date_of_census\", \n",
    "            y=\"total_individuals_in_shelter\")\n",
    "    plt.show()\n",
    "    \n",
    "except ValueError:\n",
    "    print(\"pandas could not parse the JSON.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc5e4bb",
   "metadata": {},
   "source": [
    "## 4.2 APIs \n",
    "æ¥å…¥å…¶å®ƒç¨‹åºï¼Œåœ¨ä¸çŸ¥é“æ•°æ®åº“detailsæ—¶è·å–æ•°æ®\n",
    "\n",
    "#### requests.get( url_string, params, headers)\n",
    "params è¦åˆ°API-documenté‡ŒæŸ¥çœ‹\n",
    "headers å¦‚æœæœ‰ä¿å¯†æªæ–½â€authenticationâ€œï¼Œéœ€è¦å–å¾—æˆæƒ\n",
    "#### data = response.json() è¿”å›.json æ–‡æ¡£ï¼Œå­—å…¸æ ¼å¼\n",
    "#### pd.DataFrame(data) ç”¨dataframeæ‰“å¼€jsonæ–‡æ¡£"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "975c8d69",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'api_key' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-b5f25a97dec1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Create dictionary that passes Authorization and key string, headers å’Œ paramséƒ½æ˜¯å­—å…¸æ ¼å¼çš„å‚æ•°\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mheaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'Authorization'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"Bearer {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Get data about NYC cafes from the Yelp API\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'api_key' is not defined"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd \n",
    "\n",
    "api_url = \"https://api.yelp.com/v3/businesses/search\"\n",
    "\n",
    "# Create dictionary to query API for cafes in NYC\n",
    "parameters = {'term' :'cafe',\n",
    "          \t  'location': 'NYC'}\n",
    "\n",
    "# Create dictionary that passes Authorization and key string, headers å’Œ paramséƒ½æ˜¯å­—å…¸æ ¼å¼çš„å‚æ•°\n",
    "headers = {'Authorization': \"Bearer {}\".format(api_key)}\n",
    "\n",
    "# Get data about NYC cafes from the Yelp API\n",
    "response = requests.get(api_url, \n",
    "                headers=headers, \n",
    "                params=params)\n",
    "\n",
    "# Extract JSON data from the responseï¼Œ è¿”å›çš„æ˜¯å­—å…¸æ ¼å¼ï¼Œæ‰€ä»¥ä¸èƒ½ç”¨pd.read_json(), read_json()åªèƒ½è¯»å–stringæ ¼å¼ã€‚\n",
    "data = response.json()\n",
    "\n",
    "# Load data to a data frame, é€‰å–â€˜businesses'è¿™ä¸ªkeyçš„æ‰€æœ‰values\n",
    "cafes = pd.DataFrame(data['businesses'])\n",
    " \n",
    "# View the data's dtypes\n",
    "print(cafes.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d13bf3b",
   "metadata": {},
   "source": [
    " ## 4.3 å¤„ç†Nested json\n",
    " A feature of JSON data is that it can be nested: an attribute's value can consist of attribute-value pairs. ï¼ˆè¯¦è§æœ¬è¯¾chapter4 .jsonå›¾ï¼‰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e923328",
   "metadata": {},
   "source": [
    "### Flatten nested JSONs\n",
    "##### json_normalize()   \n",
    "##### pandas.json_normalize(data:Union[Dict, List[Dict]], record_path:Union[str, List, NoneType] = None, meta:Union[str, List[Union[str, List[str]]], NoneType] = None, meta_prefix:Union[str, NoneType] = None, record_prefix:Union[str, NoneType] = None, errors:Union[str, NoneType] = 'raise', sep:str = '.', max_level:Union[int, NoneType] = None) â†’ 'DataFrame'\n",
    "\n",
    "dataï¼šdict æˆ– list of dicts\n",
    "æœªåºåˆ—åŒ–çš„JSONå°è±¡ã€‚\n",
    "\n",
    "record_pathï¼šstr æˆ– list of str, é»˜èªç‚º None\n",
    "æ¯å€‹å°è±¡åˆ°è¨˜éŒ„åˆ—è¡¨çš„è·¯å¾‘ã€‚å¦‚æœæœªé€šéï¼Œå‰‡æ•¸æ“šå°‡å‡å®šç‚ºè¨˜éŒ„æ•¸çµ„ã€‚\n",
    "\n",
    "metaï¼šlist of paths (str æˆ– list of str), é»˜èªç‚º None\n",
    "ç”¨ä½œçµæœè¡¨ä¸­æ¯å€‹è¨˜éŒ„çš„å…ƒæ•¸æ“šçš„å­—æ®µã€‚\n",
    "\n",
    "meta_prefixï¼šstr, é»˜èªç‚º None\n",
    "å¦‚æœç‚ºTrueï¼Œå‰‡å‰ç¶´è¨˜éŒ„å¸¶æœ‰é»(ï¼Ÿ)è·¯å¾‘ï¼Œä¾‹å¦‚foo.bar.fieldï¼Œå¦‚æœmetaç‚º[â€˜fooâ€™ï¼Œâ€˜barâ€™]ã€‚\n",
    "\n",
    "record_prefixï¼šstr, é»˜èªç‚º None\n",
    "å¦‚æœç‚ºTrueï¼Œå‰‡å‰ç¶´è¨˜éŒ„å¸¶æœ‰é»(ï¼Ÿ)è·¯å¾‘ï¼Œä¾‹å¦‚foo.bar.fieldï¼Œå¦‚æœè¨˜éŒ„çš„è·¯å¾‘æ˜¯[â€˜fooâ€™ï¼Œâ€˜barâ€™]ã€‚\n",
    "\n",
    "errorsï¼š{â€˜raiseâ€™, â€˜ignoreâ€™}, é»˜èªç‚º â€˜raiseâ€™\n",
    "é…ç½®éŒ¯èª¤è™•ç†ã€‚\n",
    "\n",
    "â€˜ignoreâ€™ï¼šå¦‚æœmetaä¸­åˆ—å‡ºçš„éµä¸¦éç¸½æ˜¯å­˜åœ¨ï¼Œå°‡å¿½ç•¥KeyErrorã€‚\n",
    "\n",
    "â€˜raiseâ€™ï¼šå¦‚æœmetaä¸­åˆ—å‡ºçš„éµä¸¦éç¸½æ˜¯å­˜åœ¨ï¼Œå°‡å¼•ç™¼KeyErrorã€‚\n",
    "\n",
    "sepï¼šstr, é»˜èªç‚º â€˜.â€™\n",
    "åµŒå¥—è¨˜éŒ„å°‡ç”Ÿæˆç”¨sepåˆ†éš”çš„åç¨±ã€‚ä¾‹å¦‚ï¼Œå°æ–¼sep =â€™ã€‚â€™ï¼Œ{â€˜fooâ€™ï¼š{â€˜barâ€™ï¼š0}}-> foo.barã€‚\n",
    "\n",
    "max_levelï¼šint, é»˜èªç‚º None\n",
    "è¦é€²è¡Œæ¨™æº–åŒ–çš„æœ€å¤§ç´šåˆ¥æ•¸(è©å…¸çš„æ·±åº¦)ã€‚å¦‚æœç‚ºNoneï¼Œå‰‡å°‡æ‰€æœ‰ç´šåˆ¥æ¨™æº–åŒ–ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f9ce6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load json_normalize()\n",
    "from pandas.io.json import json_normalize    \n",
    "\n",
    "# Isolate the JSON data from the API response\n",
    "data =  response.json()\n",
    "\n",
    "# Flatten business data into a data frame, replace separator\n",
    "cafes = json_normalize(data[\"businesses\"],\n",
    "             sep= '_')\n",
    "\n",
    "# View data\n",
    "print(cafes.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7fe9de",
   "metadata": {},
   "source": [
    "### Handle deeply nested data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4d9744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load other business attributes and set meta prefix\n",
    "flat_cafes = json_normalize(data[\"businesses\"],\n",
    "                            sep=\"_\",\n",
    "                    \t\trecord_path=\"categories\",\n",
    "                    \t\tmeta =['name', \n",
    "                                  'alias',  \n",
    "                                  'rating',\n",
    "                          \t\t  ['coordinates', 'latitude'], \n",
    "                          \t\t  ['coordinates', 'longitude']],\n",
    "                    \t\tmeta_prefix = \"biz_\")                    #\n",
    "\n",
    "\n",
    "# View the data\n",
    "print(flat_cafes.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a68a79e",
   "metadata": {},
   "source": [
    "## 4.4 Combining multiple datasets\n",
    "\n",
    "### 4.4.1 append() ç«–å‘ğŸ”—tables\n",
    "\n",
    "#### df1.append(df2, ignore_index = [ True #å¿½è§†pd.DataFrameçš„é»˜è®¤è¡Œindex, False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf339e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add an offset parameter to get cafes 51-100\n",
    "params = {\"term\": \"cafe\", \n",
    "          \"location\": \"NYC\",\n",
    "          \"sort_by\": \"rating\", \n",
    "          \"limit\": 50,\n",
    "          'offset': 50}\n",
    "\n",
    "result = requests.get(api_url, headers=headers, params=params)\n",
    "next_50_cafes = json_normalize(result.json()[\"businesses\"])\n",
    "\n",
    "# Append the results, setting ignore_index to renumber rows\n",
    "cafes = top_50_cafes.append(next_50_cafes, ignore_index = True)\n",
    "\n",
    "# Print shape of cafes\n",
    "print(cafes.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09a5272",
   "metadata": {},
   "source": [
    "### 4.4.2 merge() æ¨ªå‘ğŸ”—tablesï¼Œ ç±»ä¼¼SQL çš„join\n",
    "\n",
    "#### df1.merge(df2, left_on = 'df1.fk', right_on = 'df2.pk' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3637ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge crosswalk into cafes on their zip code fields\n",
    "cafes_with_pumas = cafes.merge(crosswalk, left_on = 'location_zip_code', right_on='zipcode')\n",
    "\n",
    "# Merge pop_data into cafes_with_pumas on puma field\n",
    "cafes_with_pop =  cafes_with_pumas.merge(pop_data, left_on = 'puma', right_on = 'puma')\n",
    "\n",
    "# View the data\n",
    "print(cafes_with_pop.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
