{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02a9e439",
   "metadata": {},
   "source": [
    "## 1.3.2 Communicating with an API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a7b84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = \"http://localhost:5000\"\n",
    "\n",
    "# Fill in the correct API key\n",
    "api_key = \"scientist007\"\n",
    "\n",
    "# Create the web API’s URL\n",
    "authenticated_endpoint = \"{}/{}\".format(endpoint, api_key)\n",
    "\n",
    "# Get the web API’s reply to the endpoint\n",
    "api_response = requests.get(authenticated_endpoint).json()\n",
    "pprint.pprint(api_response)\n",
    "\n",
    "# Create the API’s endpoint for the shops\n",
    "shops_endpoint = \"{}/{}/{}/{}\".format(endpoint, api_key, \"diaper/api/v1.0\", \"shops\")\n",
    "shops = requests.get(shops_endpoint).json()\n",
    "print(shops)\n",
    "\n",
    "# Create the API’s endpoint for items of the shop starting with a \"D\"\n",
    "items_of_specific_shop_URL = \"{}/{}/{}/{}/{}\".format(endpoint, api_key, \"diaper/api/v1.0\", \"items\", \"DM\")\n",
    "products_of_shop = requests.get(items_of_specific_shop_URL).json()\n",
    "pprint.pprint(products_of_shop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac1b320",
   "metadata": {},
   "source": [
    "Good job! APIs are one way of making data available to you. With the booming Internet of Things (IoT), web APIs have become more prevalent, so it’s essential to know how to interact with them and thus get data into your data lake this way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6735488",
   "metadata": {},
   "source": [
    "## 1.3.3 Streaming records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ce8fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the convenience function to query the API\n",
    "tesco_items = retrieve_products(\"Tesco\")\n",
    "\n",
    "singer.write_schema(stream_name=\"products\", schema=schema,\n",
    "                    key_properties=[])\n",
    "\n",
    "# Write a single record to the stream, that adheres to the schema\n",
    "singer.write_record(stream_name=\"products\", \n",
    "                    record={**tesco_items[0], \"store_name\": \"Tesco\"})\n",
    "\n",
    "for shop in requests.get(SHOPS_URL).json()[\"shops\"]:\n",
    "    # Write all of the records that you retrieve from the API\n",
    "    singer.write_records(\n",
    "      stream_name=\"products\", # Use the same stream name that you used in the schema\n",
    "      records=({**tesco_items[0], \"store_name\": shop}\n",
    "               for item in retrieve_products(shop))\n",
    "    )    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf458ad9",
   "metadata": {},
   "source": [
    "Using generators like you did in this exercise is a great way of keeping the memory footprint low, which is especially important when you are dealing with a lot of data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68588a00",
   "metadata": {},
   "source": [
    "## 1.3.4 Chain taps and targets\n",
    "#### 输出目标格式的提取的数据 tap =》 extract data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69dfa955",
   "metadata": {},
   "outputs": [],
   "source": [
    "tap-marketing-api | target-csv --config ingest/data_lake.conf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5466f39a",
   "metadata": {},
   "source": [
    "#### tap什么API | target输出格式 --config config文件所在文件夹位置"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8210e1",
   "metadata": {},
   "source": [
    "## 2.1.1 Read a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254d3463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read a csv file and set the headers\n",
    "df = spark.read.options(header = 'true').csv(\"/home/repl/workspace/mnt/data_lake/landing/ratings.csv\")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb753de8",
   "metadata": {},
   "source": [
    "## 2.1.2 Defining a schema\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef13dad8",
   "metadata": {},
   "source": [
    "To do this, you use classes from the pyspark.sql.types module. Its StructType() class expects a list of StructField() instances that allow you to add fields to a schema. Various other types, such as ByteType() and IntegerType() are also defined in this module and can be used to specify the data types of each field. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a6181e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the schema\n",
    "schema = StructType([\n",
    "  StructField(\"brand\", StringType(), nullable=False),\n",
    "  StructField(\"model\", StringType(), nullable=False),\n",
    "  StructField(\"absorption_rate\", ByteType(), nullable=True),\n",
    "  StructField(\"comfort\", ByteType(), nullable=True)\n",
    "])\n",
    "\n",
    "better_df = (spark\n",
    "             .read\n",
    "             .options(header=\"true\")\n",
    "             # Pass the predefined schema to the Reader\n",
    "             .schema(schema)\n",
    "             .csv(\"/home/repl/workspace/mnt/data_lake/landing/ratings.csv\"))\n",
    "pprint(better_df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f2e2f6",
   "metadata": {},
   "source": [
    "# 2.2 auto Clean  data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e0845a",
   "metadata": {},
   "source": [
    "## 2.2.1 Removing invalid rows\n",
    "Data scientists spend a lot of their time cleaning data, because most data sources they work with are not ready for analytics. Hence, the second step in the data transformation pipeline is cleaning the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c18d6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Specify the option to drop invalid rows\n",
    "ratings = (spark\n",
    "           .read\n",
    "           .options(header=\"True\", mode=\"DROPMALFORMED\")\n",
    "           .csv(\"/home/repl/workspace/mnt/data_lake/landing/ratings_with_invalid_rows.csv\"))\n",
    "ratings.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84af8d4c",
   "metadata": {},
   "source": [
    "## 2.2.2 Filling unknown data\n",
    "The .fillna() method can be used to fill null values for all columns or a subset thereof."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63af0b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"BEFORE\")\n",
    "ratings.show()\n",
    "\n",
    "print(\"AFTER\")\n",
    "# Replace nulls with arbitrary value on column subset\n",
    "ratings = ratings.fillna(4, subset=[\"comfort\"])\n",
    "ratings.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3a1b2f",
   "metadata": {},
   "source": [
    "## 2.2.3 Conditionally replacing values\n",
    "Another common situation is that you have values that you want to replace or that don’t make any sense as we saw in the video. You can select the column to be transformed by using the .withColumn() method, conditionally replace those values using the pyspark.sql.functions.when function when values meet a given condition or leave them unaltered when they don’t with the .otherwise() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6f0f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "# Add/relabel the column\n",
    "categorized_ratings = ratings.withColumn(\n",
    "    \"comfort\",\n",
    "    # Express the condition in terms of column operations\n",
    "    when(col(\"comfort\") > 3, \"sufficient\").otherwise(\"insufficient\"))\n",
    "\n",
    "categorized_ratings.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3742f77b",
   "metadata": {},
   "source": [
    "# 2.3 transformationb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5225b753",
   "metadata": {},
   "source": [
    "## 2.3.1 Selecting and renaming columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1019bde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Select the columns and rename the \"absorption_rate\" column\n",
    "result = ratings.select([col(\"brand\"),\n",
    "                       col(\"model\"),\n",
    "                       col(\"absorption_rate\").alias(\"absorbency\")])\n",
    "\n",
    "# Show only unique values\n",
    "result.distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b522d8",
   "metadata": {},
   "source": [
    "## 2.3.2 Grouping and aggregating data\n",
    "\n",
    "you’ll use the spark.sql aggregation functions avg() to compute the average value of some column in a group, stddev_samp() to compute the standard (sample) deviation and max() (which we alias as sfmax so as not to shadow Python’s built-in max()) to retrieve the largest value of some column in a group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33bbd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, avg, stddev_samp, max as sfmax\n",
    "\n",
    "aggregated = (purchased\n",
    "              # Group rows by 'Country'\n",
    "              .groupBy(col('Country'))\n",
    "              .agg(\n",
    "                # Calculate the average salary per group and rename\n",
    "                avg('Salary').alias('average_salary'),\n",
    "                # Calculate the standard deviation per group\n",
    "                stddev_samp('Salary'),\n",
    "                # Retain the highest salary per group and rename\n",
    "                sfmax('Salary').alias('highest_salary')\n",
    "              )\n",
    "             )\n",
    "\n",
    "aggregated.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ef0874",
   "metadata": {},
   "source": [
    "# 2.4 packaging and submit application\n",
    "\n",
    "编写好的Spark程序一般通过Spark-submit指令的方式提交给Spark集群进行具体的任务计算\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1e9cd7",
   "metadata": {},
   "source": [
    "## 2.4.1 zip spark file\n",
    "\n",
    "You archived all of the code in a single zip file, which can be passed to the spark-submit command for distribution in a compute cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3255ed89",
   "metadata": {},
   "outputs": [],
   "source": [
    "zip --recurse-paths zip_file.zip pipeline_folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ee0f61",
   "metadata": {},
   "source": [
    "## 2.4.2 Submitting your Spark job\n",
    "\n",
    "With the dependencies of a job ready to be distributed across a cluster’s nodes, you can submit a job to a cluster easily. In this exercise, you'll be testing the job locally.\n",
    "\n",
    "spark-submit --py-files PY_FILES MAIN_PYTHON_FILE\n",
    "\n",
    "with PY_FILES being either a zipped archive, a Python egg or separate Python files that will be placed on the PYTHONPATH environment variable of your cluster's nodes. The MAIN_PYTHON_FILE should be the entry point of your application.\n",
    "\n",
    "In this particular exercise, the path of the zipped archive is spark_pipelines/pydiaper/pydiaper.zip whereas the path to your application entry point is spark_pipelines/pydiaper/pydiaper/cleaning/clean_ratings.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f39d04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shell commond\n",
    "spark-submit --py-files spark_pipelines/pydiaper/pydiaper.zip spark_pipelines/pydiaper/pydiaper/cleaning/clean_ratings.py."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15ccb1f",
   "metadata": {},
   "source": [
    "# 3.2 test "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a532d491",
   "metadata": {},
   "source": [
    "## 3.2.1 Creating in-memory DataFrames\n",
    "Creating small datasets for unit tests is an important skill. It improves readability and understanding, because any developer looking at your code, can immediately see the inputs to some function and how they relate to the output. Additionally, you can illustrate how the function behaves with normal data and with exceptional data, like missing or incorrect fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0180d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "from pyspark.sql import Row\n",
    "\n",
    "Record = Row(\"country\", \"utm_campaign\", \"airtime_in_minutes\", \"start_date\", \"end_date\")\n",
    "\n",
    "# Create a tuple of records\n",
    "data = (\n",
    "  Record(\"USA\", \"DiapersFirst\", 28, date(2017, 1, 20), date(2017, 1, 27)),\n",
    "  Record(\"Germany\", \"WindelKind\", 31, date(2017, 1, 25), None),\n",
    "  Record(\"India\", \"CloseToCloth\", 32, date(2017, 1, 25), date(2017, 2, 2))\n",
    ")\n",
    "\n",
    "# Create a DataFrame from these records\n",
    "frame = spark.createDataFrame(data)\n",
    "frame.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f01e90ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0  1  2\n",
      "0  1  2  3\n",
      "1  a  b  c\n",
      "RangeIndex(start=0, stop=2, step=1)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'row'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-50254b63cabc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5272\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5273\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5274\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5276\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'row'"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
