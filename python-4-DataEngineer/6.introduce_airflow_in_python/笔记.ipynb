{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36063b2d",
   "metadata": {},
   "source": [
    "# 1.创建DAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343c4a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.models import DAG\n",
    "from airflow.operators.bash_operator import BashOperator\n",
    "from datetime import datetime\n",
    "\n",
    "default_args = {\n",
    "  'owner': 'dsmith',\n",
    "  'start_date': datetime(2020, 2, 12),\n",
    "  'retries': 1\n",
    "}\n",
    "\n",
    "codependency_dag = DAG('codependency', default_args=default_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c5229f",
   "metadata": {},
   "source": [
    "# 2.1 创建bash operator， 并用bitshift 规划downstream，upstream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43ec344",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "task1 = BashOperator(task_id='first_task',\n",
    "                     bash_command='echo 1',\n",
    "                     dag=codependency_dag)\n",
    "\n",
    "task2 = BashOperator(task_id='second_task',\n",
    "                     bash_command='echo 2',\n",
    "                     dag=codependency_dag)\n",
    "\n",
    "task3 = BashOperator(task_id='third_task',\n",
    "                     bash_command='echo 3',\n",
    "                     dag=codependency_dag)\n",
    "\n",
    "# task1 must run before task2 which must run before task3\n",
    "task1 >> task2\n",
    "task2 >> task3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bd97e3",
   "metadata": {},
   "source": [
    "# 2.2 创建 PythonOperator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c8a973",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_file(URL, savepath):\n",
    "    r = requests.get(URL)\n",
    "    with open(savepath, 'wb') as f:\n",
    "        f.write(r.content)   \n",
    "    # Use the print method for logging\n",
    "    print(f\"File pulled from {URL} and saved to {savepath}\")\n",
    "\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "\n",
    "# Create the task\n",
    "pull_file_task = PythonOperator(\n",
    "    task_id ='pull_file',\n",
    "    # Add the callable\n",
    "    python_callable=pull_file,\n",
    "    # Define the arguments\n",
    "    op_kwargs ={'URL':'http://dataserver/sales.json', 'savepath':'latestsales.json'},\n",
    "    dag=process_sales_dag\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82f9ac4",
   "metadata": {},
   "source": [
    "# 2.3 创建EmailOperator， 自动给用户发邮件监控数据工作情况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ada0ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Operator\n",
    "from airflow.operators.email_operator import EmailOperator \n",
    "\n",
    "# Define the task\n",
    "email_manager_task = EmailOperator(\n",
    "    task_id='email_manager',\n",
    "    to='manager@datacamp.com',\n",
    "    subject ='Latest sales JSON',\n",
    "    html_content='Attached is the latest sales JSON file as requested.',\n",
    "    files  ='parsedfile.json',\n",
    "    dag=process_sales_dag\n",
    ")\n",
    "\n",
    "# Set the order of tasks\n",
    "pull_file_task >> parse_file_task >> email_manager_task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921a511f",
   "metadata": {},
   "source": [
    "# 2.4 scheduling\n",
    "\n",
    "### schedule_interval = '@day 或 cron syntax'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1664fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the scheduling arguments as defined\n",
    "default_args = {\n",
    "  'owner': 'Engineering',\n",
    "  'start_date': datetime(2019, 11, 1),              # 在创建DAG的时候，创建DAG开始运行日期\n",
    "  'email': ['airflowresults@datacamp.com'],\n",
    "  'email_on_failure': False,\n",
    "  'email_on_retry': False,\n",
    "  'retries': 3,\n",
    "  'retry_delay': timedelta(minutes=20)\n",
    "}\n",
    "\n",
    "dag = DAG('update_dataflows', default_args=default_args, schedule_interval='30 12 * * 3')  # schedule_interval 创建DAG更新间隔"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6293d3",
   "metadata": {},
   "source": [
    "# 3.1 sensors \n",
    "## 感应一些条件是否达成，从而推进task进入到下一个task\n",
    "#### 结构和operator类似， 有3个参数\n",
    "#### 1.mode = 'poke' (反复监控一个条件，只有达成才进下一步） 或 'reschedule'（会放弃达成不了的条件，进行下一步，try later）\n",
    "#### 2.poke_interval 等多久监控一次\n",
    "#### 3.timeout 超过多久就算失败\n",
    "\n",
    "## 除了FileSensor, 还有 ExternalTaskSensor， SqlSensor, HttpSensor 等等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d84ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.contrib.sensors.file_sensor import FileSensor\n",
    "    \n",
    "precheck = FileSensor(\n",
    "    task_id='check_for_datafile',\n",
    "    filepath='salesdata_ready.csv',\n",
    "    start_date=datetime(2020,2,20),\n",
    "    mode='reschedule',\n",
    "    dag=report_dag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa99f6f",
   "metadata": {},
   "source": [
    "# 3.2 Executor\n",
    "### 1. SequentialExecutor  一次只跑一个task，用于debug\n",
    "### 2. CeleryExecutor\n",
    "### 3. LocalExecutor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7922cbb",
   "metadata": {},
   "source": [
    "通过bash command: airflow list_dags 查看"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2edab2b7",
   "metadata": {},
   "source": [
    "# 3.3 常见的错误，以及debugging 和 troubleshooting "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86329563",
   "metadata": {},
   "source": [
    "# 3.4 SLA( service level agreement 检测SLA丢失情况) 和 reporting (通过email） \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4e56a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the timedelta object\n",
    "from datetime import timedelta\n",
    "\n",
    "# Create the dictionary entry\n",
    "default_args = {\n",
    "  'start_date': datetime(2020, 2, 20),\n",
    "  'sla': timedelta(minutes = 30)\n",
    "}\n",
    "\n",
    "# Add to the DAG\n",
    "test_dag = DAG('test_workflow', default_args=default_args, schedule_interval='@None')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e532231f",
   "metadata": {},
   "source": [
    "##### Remember that this type of SLA applies for the entire workflow, not just an individual task.\n",
    "####  you can add specific SLAs to individual tasks as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b176b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the timedelta object\n",
    "from datetime import timedelta\n",
    "\n",
    "test_dag = DAG('test_workflow', start_date=datetime(2020,2,20), schedule_interval='@None')\n",
    "\n",
    "# Create the task with the SLA\n",
    "task1 = BashOperator(task_id='first_task',\n",
    "                     sla =timedelta(hours = 3),\n",
    "                     bash_command='initialize_data.sh',\n",
    "                     dag=test_dag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c10477",
   "metadata": {},
   "source": [
    "## reporting\n",
    "#### You would like to receive a report from Airflow when tasks complete without requiring constant monitoring of the UI or log files. You decide to use the email functionality within Airflow to provide this message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8e4e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the email task\n",
    "email_report = EmailOperator(\n",
    "        task_id='email_report',\n",
    "        to='airflow@datacamp.com',\n",
    "        subject='Airflow Monthly Report',\n",
    "        html_content=\"\"\"Attached is your monthly workflow report - please refer to it for more detail\"\"\",\n",
    "        files=['monthly_report.pdf'],\n",
    "        dag=report_dag\n",
    ")\n",
    "\n",
    "# Set the email task to run after the report is generated\n",
    "email_report << generate_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fddb5b7",
   "metadata": {},
   "source": [
    "#### 也可以在dag中default args中设置，报告airflow run成功或失败\n",
    "#### Use these options in production to monitor the state of your workflows to help avoid surprises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373f9108",
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.models import DAG\n",
    "from airflow.operators.bash_operator import BashOperator\n",
    "from airflow.contrib.sensors.file_sensor import FileSensor\n",
    "from datetime import datetime\n",
    "\n",
    "default_args={\n",
    "    'email': ['airflowalerts@datacamp.com','airflowadmin@datacamp.com'],\n",
    "    'email_on_failure': True,\n",
    "    'email_on_success': True \n",
    "}\n",
    "report_dag = DAG(\n",
    "    dag_id = 'execute_report',\n",
    "    schedule_interval = \"0 0 * * *\",\n",
    "    default_args=default_args\n",
    ")\n",
    "\n",
    "precheck = FileSensor(\n",
    "    task_id='check_for_datafile',\n",
    "    filepath='salesdata_ready.csv',\n",
    "    start_date=datetime(2020,2,20),\n",
    "    mode='reschedule',\n",
    "    dag=report_dag)\n",
    "\n",
    "generate_report_task = BashOperator(\n",
    "    task_id='generate_report',\n",
    "    bash_command='generate_report.sh',\n",
    "    start_date=datetime(2020,2,20),\n",
    "    dag=report_dag\n",
    ")\n",
    "\n",
    "precheck >> generate_report_task\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db41cc76",
   "metadata": {},
   "source": [
    "# 4.1 templates\n",
    "## 偷懒用的，减少bash_command 的重复性输入，做一个模板，嵌套要插入的变量，一劳永逸\n",
    "\n",
    "#### use a templated command instead of hardcoding your workflow objects. This will come in very handy when creating production workflows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8998a5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    " templated_command=\"\"\"\n",
    "  echo \"Reading {{ params.filename }}\"\n",
    "\"\"\"\n",
    "t1 = BashOperator(task_id='template_task',\n",
    "       bash_command=templated_command,\n",
    "       params={'filename': 'file1.txt'}\n",
    "       dag=example_dag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125f21cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "default_args = {\n",
    "  'start_date': datetime(2020, 4, 15),\n",
    "}\n",
    "\n",
    "cleandata_dag = DAG('cleandata',\n",
    "                    default_args=default_args,\n",
    "                    schedule_interval='@daily')\n",
    "\n",
    "# Modify the templated command to handle a\n",
    "# second argument called filename. 两个参数，一个变量params\n",
    "templated_command = \"\"\"\n",
    "  bash cleandata.sh {{ ds_nodash }} {{params.filename}} \n",
    "\"\"\"\n",
    "\n",
    "# Modify clean_task to pass the new argument\n",
    "clean_task = BashOperator(task_id='cleandata_task',\n",
    "                          bash_command=templated_command,\n",
    "                          params={'filename': 'salesdata.txt'},  #每个task不同的unique 变量\n",
    "                          dag=cleandata_dag)\n",
    "\n",
    "# Create a new BashOperator clean_task2\n",
    "clean_task2 = BashOperator(task_id='cleandata_task2',\n",
    "                           bash_command = templated_command ,\n",
    "                           params = {'filename' : 'supportdata.txt'},\n",
    "                           dag = cleandata_dag  )\n",
    "                           \n",
    "# Set the operator dependencies\n",
    "clean_task2 << clean_task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fc0c97",
   "metadata": {},
   "source": [
    "# 4.2 Using lists with templates\n",
    "\n",
    "##### This time, you realize that you need to run the command cleandata.sh with the date argument and the file argument as before, except now you have a list of 30 files. You do not want to create 30 tasks, so your job is to modify the code to support running the argument for 30 or more files.\n",
    "\n",
    "#### When using a single task, all entries would succeed or fail as a single task. Separate operators allow for better monitoring and scheduling of these tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb349e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.models import DAG\n",
    "from airflow.operators.bash_operator import BashOperator\n",
    "from datetime import datetime\n",
    "\n",
    "filelist = [f'file{x}.txt' for x in range(30)]\n",
    "\n",
    "default_args = {\n",
    "  'start_date': datetime(2020, 4, 15),\n",
    "}\n",
    "\n",
    "cleandata_dag = DAG('cleandata',\n",
    "                    default_args=default_args,\n",
    "                    schedule_interval='@daily')\n",
    "\n",
    "# Modify the template to handle multiple files in a \n",
    "# single run.\n",
    "templated_command = \"\"\"\n",
    "  <% for filename in params.filenames %>\n",
    "  bash cleandata.sh {{ ds_nodash }} {{ filename }};\n",
    "  <% endfor %>\n",
    "\"\"\"\n",
    "\n",
    "# Modify clean_task to use the templated command\n",
    "clean_task = BashOperator(task_id='cleandata_task',\n",
    "                          bash_command=templated_command,\n",
    "                          params={'filenames': filelist},\n",
    "                          dag=cleandata_dag)\n",
    "                          \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6d484d",
   "metadata": {},
   "source": [
    "# 4.3 branching 分流做分支判断，进入到不同的流程\n",
    "## BranchPythonOperator\n",
    "#### This is a simple but effective use of branching to perform an occasional set of tasks without requiring significant code changes. Make sure to remember the various capabilities with branching to make your workflows more robust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d39537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to determine if years are different\n",
    "def year_check(**kwargs):\n",
    "    current_year = int(kwargs['ds_nodash'][0:4])\n",
    "    previous_year = int(kwargs['prev_ds_nodash'][0:4])\n",
    "    if current_year == previous_year:\n",
    "        return 'current_year_task'\n",
    "    else:\n",
    "        return 'new_year_task'\n",
    "\n",
    "# Define the BranchPythonOperator\n",
    "branch_task = BranchPythonOperator(task_id='branch_task', dag=branch_dag,\n",
    "                                   python_callable=year_check, provide_context=True)\n",
    "# Define the dependencies\n",
    "branch_dag >> current_year_task\n",
    "branch_dag >> new_year_task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a457d59",
   "metadata": {},
   "source": [
    "# 4.4 ☆一个完整的airflow Pipline建造代码☆\n",
    "\n",
    "Operators, tasks, sensors, conditional logic, templating, SLAs, dependencies, and even alerting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8ee700",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "\n",
    "def process_data(**kwargs):\n",
    "    file = open(\"/home/repl/workspace/processed_data-\" + kwargs['ds'] + \".tmp\", \"w\")\n",
    "    file.write(f\"Data processed on {date.today()}\")\n",
    "    file.close()\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8190fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.models import DAG\n",
    "from airflow.contrib.sensors.file_sensor import FileSensor\n",
    "from airflow.operators.bash_operator import BashOperator\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "from airflow.operators.python_operator import BranchPythonOperator\n",
    "from airflow.operators.dummy_operator import DummyOperator\n",
    "from airflow.operators.email_operator import EmailOperator\n",
    "from dags.process import process_data\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Update the default arguments and apply them to the DAG.\n",
    "\n",
    "default_args = {\n",
    "  'start_date': datetime(2019,1,1),\n",
    "  'sla': timedelta(minutes=90)\n",
    "}\n",
    "    \n",
    "dag = DAG(dag_id='etl_update', default_args=default_args)\n",
    "\n",
    "sensor = FileSensor(task_id='sense_file', \n",
    "                    filepath='/home/repl/workspace/startprocess.txt',\n",
    "                    poke_interval=45,\n",
    "                    dag=dag)\n",
    "\n",
    "bash_task = BashOperator(task_id='cleanup_tempfiles', \n",
    "                         bash_command='rm -f /home/repl/*.tmp',\n",
    "                         dag=dag)\n",
    "\n",
    "python_task = PythonOperator(task_id='run_processing', \n",
    "                             python_callable=process_data,\n",
    "                             provide_context=True,\n",
    "                             dag=dag)\n",
    "\n",
    "\n",
    "email_subject=\"\"\"\n",
    "  Email report for {{ params.department }} on {{ ds_nodash }}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "email_report_task = EmailOperator(task_id='email_report_task',\n",
    "                                  to='sales@mycompany.com',\n",
    "                                  subject=email_subject,\n",
    "                                  html_content='',\n",
    "                                  params={'department': 'Data subscription services'},\n",
    "                                  dag=dag)\n",
    "\n",
    "\n",
    "no_email_task = DummyOperator(task_id='no_email_task', dag=dag)\n",
    "\n",
    "\n",
    "def check_weekend(**kwargs):\n",
    "    dt = datetime.strptime(kwargs['execution_date'],\"%Y-%m-%d\")\n",
    "    # If dt.weekday() is 0-4, it's Monday - Friday. If 5 or 6, it's Sat / Sun.\n",
    "    if (dt.weekday() < 5):\n",
    "        return 'email_report_task'\n",
    "    else:\n",
    "        return 'no_email_task'\n",
    "    \n",
    "    \n",
    "branch_task = BranchPythonOperator(task_id='check_if_weekend',\n",
    "                                   python_callable = check_weekend,\n",
    "                                   provide_context = True,\n",
    "                                   dag=dag)\n",
    "\n",
    "    \n",
    "sensor >> bash_task >> python_task\n",
    "\n",
    "python_task >> branch_task >> [email_report_task, no_email_task]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
